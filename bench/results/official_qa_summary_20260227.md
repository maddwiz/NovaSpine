# NovaSpine Official QA Summary (2026-02-27)

Configuration:
- `--top-k 10`
- `--ingest-sync`
- `--skip-chunking`
- `--answer-mode extractive`
- embeddings: default provider (keyword-led retrieval path)

| Benchmark | Questions | Doc Hit Rate | Exact Match | Token F1 |
|---|---:|---:|---:|---:|
| LongMemEval | 500 | 1.000 | 0.044 | 0.123 |
| LoCoMo-MC10 (sample-500 source rows) | 143 | 0.860 | 0.021 | 0.074 |
| DMR sample-500 | 500 | 0.706 | 0.012 | 0.037 |

Notes:
- Retrieval remains strong (especially LongMemEval and LoCoMo doc-hit), while extractive answer generation is still the primary bottleneck.
- This QA pass is fully reproducible from `scripts/run_memory_qa.py` and converted QA datasets generated by `scripts/prepare_official_benchmarks.py`.

---

Configuration:
- `--top-k 10`
- `--ingest-sync`
- `--skip-chunking`
- `--answer-mode llm`
- `--answer-provider openai`
- `--answer-model gpt-4.1-mini`
- `--answer-context-k 8`
- `--answer-max-context-chars 12000`
- `--answer-max-tokens 96`

| Benchmark | Questions | Doc Hit Rate | Exact Match | Token F1 |
|---|---:|---:|---:|---:|
| LongMemEval (OpenAI LLM QA) | 500 | 1.000 | 0.150 | 0.286 |
| LoCoMo-MC10 (OpenAI LLM QA) | 143 | 0.860 | 0.294 | 0.455 |
| DMR sample-500 (OpenAI LLM QA) | 500 | 0.706 | 0.396 | 0.529 |

Delta vs extractive baseline:
- LongMemEval: `EM +0.106`, `F1 +0.162`
- LoCoMo-MC10: `EM +0.273`, `F1 +0.381`
- DMR sample-500: `EM +0.384`, `F1 +0.492`

OpenAI outputs:
- `bench/results/official_longmemeval_qa_openai_20260227.json`
- `bench/results/official_locomo_qa_openai_20260227.json`
- `bench/results/official_dmr_qa_openai_20260227.json`
